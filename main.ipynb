{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mcg43.png</td>\n",
       "      <td>mcg43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c4bgd.png</td>\n",
       "      <td>c4bgd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cdfen.png</td>\n",
       "      <td>cdfen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ne325.png</td>\n",
       "      <td>ne325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bpwd7.png</td>\n",
       "      <td>bpwd7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    img_path  label\n",
       "0  mcg43.png  mcg43\n",
       "1  c4bgd.png  c4bgd\n",
       "2  cdfen.png  cdfen\n",
       "3  ne325.png  ne325\n",
       "4  bpwd7.png  bpwd7"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "directory_path = './data/samples'\n",
    "\n",
    "def get_file_names(directory):\n",
    "    file_names = []\n",
    "    for filename in os.listdir(directory):\n",
    "        file_names.append(filename)\n",
    "    return file_names\n",
    "\n",
    "# 파일명 리스트 얻기\n",
    "file_names = get_file_names(directory_path)\n",
    "\n",
    "characters = set()\n",
    "captcha_length = []\n",
    "dataset = []\n",
    "\n",
    "for img_path in file_names:\n",
    "    label = img_path.split(\".\")[0]\n",
    "    captcha_length.append(len(label))\n",
    "    dataset.append((str(img_path), label))\n",
    "    for ch in label:\n",
    "        characters.add(ch)\n",
    "\n",
    "import pandas as pd\n",
    "characters = sorted(characters)\n",
    "dataset = pd.DataFrame(dataset, columns=[\"img_path\", \"label\"], index=None)\n",
    "dataset = dataset.sample(frac=1.).reset_index(drop=True)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'm',\n",
       " 'n',\n",
       " 'p',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "seed = 123\n",
    "training_data, validation_data = train_test_split(dataset, test_size=0.2, random_state=seed)\n",
    "\n",
    "training_data = training_data.reset_index(drop=True)\n",
    "validation_data = validation_data.reset_index(drop=True)\n",
    "\n",
    "char_to_labels = {char:idx for idx, char in enumerate(characters)}\n",
    "labels_to_char = {val:key for key, val in char_to_labels.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def generate_arrays(df, resize=True, img_height=50, img_width=200):    \n",
    "    num_items = len(df)\n",
    "    images = np.zeros((num_items, img_height, img_width), dtype=np.float32)\n",
    "    labels = [0]*num_items\n",
    "    \n",
    "    for i in range(num_items):\n",
    "        img = cv2.imread(df[\"img_path\"][i])\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        if resize: \n",
    "            img = cv2.resize(img, (img_width, img_height))\n",
    "        \n",
    "        img = (img/255.).astype(np.float32)\n",
    "        label = df[\"label\"][i]\n",
    "        \n",
    "        images[i, :, :] = img\n",
    "        labels[i] = label\n",
    "    \n",
    "    return images, np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@54.369] global loadsave.cpp:248 findDecoder imread_('nf8b8.png'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) /io/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m training_data, training_labels \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m validation_data, validation_labels \u001b[38;5;241m=\u001b[39m generate_arrays(df\u001b[38;5;241m=\u001b[39mvalidation_data)\n",
      "Cell \u001b[0;32mIn[21], line 11\u001b[0m, in \u001b[0;36mgenerate_arrays\u001b[0;34m(df, resize, img_height, img_width)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_items):\n\u001b[1;32m     10\u001b[0m     img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg_path\u001b[39m\u001b[38;5;124m\"\u001b[39m][i])\n\u001b[0;32m---> 11\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2GRAY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resize: \n\u001b[1;32m     14\u001b[0m         img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(img, (img_width, img_height))\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.9.0) /io/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "training_data, training_labels = generate_arrays(df=training_data)\n",
    "validation_data, validation_labels = generate_arrays(df=validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from module.custom_dataset import CaptchaDataset\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import CTCLoss\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 데이터셋 로드 및 DataLoader 설정\n",
    "train_dataset = CaptchaDataset('./data/samples', transform=transforms.ToTensor())\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# 모델 설정\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "model.classifier = nn.Linear(model.classifier.in_features, len(train_dataset))  # 새 분류 레이어\n",
    "\n",
    "# 훈련 루프\n",
    "num_epochs = 10\n",
    "checkpoint_interval = 2  # 체크포인트 저장 주기 (2 에포크마다)\n",
    "\n",
    "\n",
    "# 손실 함수 및 옵티마이저 설정\n",
    "criterion = CTCLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    # tqdm을 사용하여 진행 상황을 추적\n",
    "    progress = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}')\n",
    "\n",
    "    for inputs, labels in progress:\n",
    "        inputs = torch.stack(inputs)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # tqdm 업데이트\n",
    "        progress.set_postfix({'Loss': loss.item()})\n",
    "\n",
    "    # 체크포인트 저장\n",
    "    if (epoch + 1) % checkpoint_interval == 0:\n",
    "        checkpoint_path = f\"checkpoint_epoch_{epoch + 1}.pth\"\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"Checkpoint saved at {checkpoint_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "captcha-reading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
